{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HX2ETB09FQGR"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from os.path import isfile\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Bidirectional, LSTM, Dropout, Activation, GRU\n",
    "from keras.layers import Conv2D, concatenate, MaxPooling2D, Flatten, Embedding, Lambda\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "def conv_recurrent_model_build(model_input):\n",
    "    print('Building model...')\n",
    "    layer = model_input\n",
    "    \n",
    "    ### Convolutional blocks\n",
    "    conv_1 = Conv2D(filters = nb_filters1, \n",
    "                    kernel_size = ksize, \n",
    "                    strides=1,\n",
    "                    padding= 'valid', \n",
    "                    activation='relu', \n",
    "                    name='conv_1')(layer)\n",
    "    \n",
    "    pool_1 = MaxPooling2D(pool_size_1)(conv_1)\n",
    "\n",
    "    conv_2 = Conv2D(filters = nb_filters2, \n",
    "                    kernel_size = ksize, \n",
    "                    strides=1,\n",
    "                    padding= 'valid', \n",
    "                    activation='relu', \n",
    "                    name='conv_2')(pool_1)\n",
    "    \n",
    "    pool_2 = MaxPooling2D(pool_size_1)(conv_2)\n",
    "\n",
    "    conv_3 = Conv2D(filters = nb_filters3, \n",
    "                    kernel_size = ksize, \n",
    "                    strides=1,\n",
    "                    padding= 'valid', \n",
    "                    activation='relu', \n",
    "                    name='conv_3')(pool_2)\n",
    "\n",
    "    pool_3 = MaxPooling2D(pool_size_2)(conv_3)\n",
    "\n",
    "    flatten1 = Flatten()(pool_3)\n",
    "    ### Recurrent Block\n",
    "    \n",
    "    # Pooling layer\n",
    "    pool_lstm1 = MaxPooling2D(pool_size_3, \n",
    "                              name = 'pool_lstm')(layer)\n",
    "    \n",
    "    # Embedding layer\n",
    "    squeezed = Lambda(lambda x: K.squeeze(x, axis= -1))(pool_lstm1)\n",
    "\n",
    "    # Bidirectional GRU\n",
    "    # default merge mode is concat\n",
    "    lstm = Bidirectional(GRU(lstm_count))(squeezed)  \n",
    "    \n",
    "    # Concat Output\n",
    "    concat = concatenate([flatten1, lstm], \n",
    "                         axis=-1, \n",
    "                         name ='concat')\n",
    "    \n",
    "    ## Softmax Output\n",
    "    output = Dense(num_classes, \n",
    "                   activation = 'softmax', \n",
    "                   name='preds')(concat)\n",
    "    \n",
    "    model_output = output\n",
    "    model = Model(model_input, model_output)\n",
    "    \n",
    "    opt = RMSprop(lr=0.0005)  # Optimizer\n",
    "    model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model\n",
    "  \n",
    "def train_model(x_train, y_train, x_val, y_val):\n",
    "    \n",
    "    n_frequency = 43\n",
    "    n_frames = 256\n",
    "    \n",
    "    x_train = np.expand_dims(x_train, axis = -1)\n",
    "    x_val = np.expand_dims(x_val, axis = -1)\n",
    "  \n",
    "    weight = np.ones((y_train.shape[0]))\n",
    "    prog_len = len(y_train[y_train[:,0]==1])\n",
    "    totallen = y_train.shape[0]\n",
    "    weight[y_train[:,0] == 0] = prog_len/totallen\n",
    "    weight[y_train[:,0] == 1] = 1 - prog_len/totallen\n",
    "    sample_weight = weight\n",
    "    \n",
    "    input_shape = (n_frames, n_frequency, 1)\n",
    "    model_input = Input(input_shape, name='input')\n",
    "    \n",
    "    model = conv_recurrent_model_build(model_input)\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint('/path/weights.best.h5', \n",
    "                                          monitor='val_acc', \n",
    "                                          verbose=1,\n",
    "                                          save_best_only=True, \n",
    "                                          mode='max')\n",
    "    \n",
    "    reducelr_callback = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                          factor=0.5, \n",
    "                                          patience=10, \n",
    "                                          min_delta=0.01,\n",
    "                                          verbose=1)\n",
    "    \n",
    "    callbacks_list = [checkpoint_callback, reducelr_callback]\n",
    "\n",
    "    # Fit the model and get training history.\n",
    "    print('Training...')\n",
    "    history = model.fit(x_train, \n",
    "                        y_train, \n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        epochs=EPOCH_COUNT,\n",
    "                        sample_weight = sample_weight,\n",
    "                        validation_data=(x_val, y_val), \n",
    "                        verbose=1, \n",
    "                        callbacks=callbacks_list)\n",
    "\n",
    "    return model, history\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MLHandIn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
